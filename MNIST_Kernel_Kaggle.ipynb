{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working from this [kernel in Kaggle](https://www.kaggle.com/sdelecourt/cnn-with-pytorch-for-mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # to handle matrix and data operation\n",
    "import pandas as pd # to read csv and handle dataframe\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to keep training parameters in the same place\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('digit-recognizer/train.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label'].values\n",
    "X = df.drop(['label'],1).values # Drop the labels so you don't get data pollution\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15) # Split into test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_X_train = torch.from_numpy(X_train).type(torch.LongTensor)\n",
    "torch_y_train = torch.from_numpy(y_train).type(torch.LongTensor) # data type is long; But why?\n",
    "\n",
    "# create feature and targets tensor for test set\n",
    "torch_X_test = torch.from_numpy(X_test).type(torch.LongTensor)\n",
    "torch_y_test = torch.from_numpy(y_test).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\n",
    "test = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the data flow in torch in this example goes follows:\n",
    "1. Read the data in from csv file\n",
    "2. Convert the data to numpy array (can be combined with step 1)\n",
    "3. Split the dataset into test and train data, using scikit learn functionality\n",
    "4. Convert the test and train datasets into *Torch Tensor*\n",
    "5. Convert the *Torch Tensors* into *Tensor Datasets* for both train and test data\n",
    "6. Input the Datasets into a data loader, considering a specified batch size\n",
    "\n",
    "Seems like an unnecessary number of steps; Chance to optimize, at the very least for shorter and clearer code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear1): Linear(in_features=784, out_features=250, bias=True)\n",
      "  (linear2): Linear(in_features=250, out_features=100, bias=True)\n",
      "  (linear3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Same as other example; Create an inherited class for the neural network\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(784,250)\n",
    "        self.linear2 = nn.Linear(250,100)\n",
    "        self.linear3 = nn.Linear(100,10) # Linear means fully-connected layer\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        return F.log_softmax(X, dim=1)\n",
    " \n",
    "mlp = MLP()\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'*' Copied from the example code\n",
    "\n",
    "Keep in mind that none of this network involves convolution; All layers are fully-connected\n",
    "\n",
    "We have 784*(250+1) + 250*(100+1) + 100*(10+1) = 222 360 parameters to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader):\n",
    "    # Adam is a method for stochastic gradient descent; avail. below\n",
    "    # https://pytorch.org/docs/stable/_modules/torch/optim/adam.html\n",
    "    optimizer = torch.optim.Adam(model.parameters())#,lr=0.001, betas=(0.9,0.999))\n",
    "    error = nn.CrossEntropyLoss() # Cross Entropy loss is a loss fxn, AKA log loss\n",
    "    # Read more about cross entropy, or log loss, here: https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html\n",
    "    model.train() # Calling .train() method on nn.Module object\n",
    "    for epoch in range(EPOCHS):\n",
    "        correct = 0\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(var_X_batch)\n",
    "            loss = error(output, var_y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Total correct predictions\n",
    "            predicted = torch.max(output.data, 1)[1] \n",
    "            correct += (predicted == var_y_batch).sum()\n",
    "            # print(correct)\n",
    "            if batch_idx % 50 == 0:\n",
    "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.item(), float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/35700 (0%)]\tLoss: 0.091644\t Accuracy:96.875%\n",
      "Epoch : 0 [1600/35700 (4%)]\tLoss: 0.536852\t Accuracy:96.507%\n",
      "Epoch : 0 [3200/35700 (9%)]\tLoss: 0.028471\t Accuracy:96.349%\n",
      "Epoch : 0 [4800/35700 (13%)]\tLoss: 0.083459\t Accuracy:96.482%\n",
      "Epoch : 0 [6400/35700 (18%)]\tLoss: 0.305395\t Accuracy:96.315%\n",
      "Epoch : 0 [8000/35700 (22%)]\tLoss: 0.450834\t Accuracy:96.352%\n",
      "Epoch : 0 [9600/35700 (27%)]\tLoss: 0.045381\t Accuracy:96.408%\n",
      "Epoch : 0 [11200/35700 (31%)]\tLoss: 0.003097\t Accuracy:96.359%\n",
      "Epoch : 0 [12800/35700 (36%)]\tLoss: 0.199553\t Accuracy:96.462%\n",
      "Epoch : 0 [14400/35700 (40%)]\tLoss: 0.106324\t Accuracy:96.529%\n",
      "Epoch : 0 [16000/35700 (45%)]\tLoss: 0.255821\t Accuracy:96.544%\n",
      "Epoch : 0 [17600/35700 (49%)]\tLoss: 0.060047\t Accuracy:96.586%\n",
      "Epoch : 0 [19200/35700 (54%)]\tLoss: 0.047019\t Accuracy:96.553%\n",
      "Epoch : 0 [20800/35700 (58%)]\tLoss: 0.047048\t Accuracy:96.597%\n",
      "Epoch : 0 [22400/35700 (63%)]\tLoss: 0.005376\t Accuracy:96.594%\n",
      "Epoch : 0 [24000/35700 (67%)]\tLoss: 0.119014\t Accuracy:96.605%\n",
      "Epoch : 0 [25600/35700 (72%)]\tLoss: 0.201614\t Accuracy:96.578%\n",
      "Epoch : 0 [27200/35700 (76%)]\tLoss: 0.047364\t Accuracy:96.585%\n",
      "Epoch : 0 [28800/35700 (81%)]\tLoss: 0.161868\t Accuracy:96.594%\n",
      "Epoch : 0 [30400/35700 (85%)]\tLoss: 0.174228\t Accuracy:96.589%\n",
      "Epoch : 0 [32000/35700 (90%)]\tLoss: 0.028402\t Accuracy:96.585%\n",
      "Epoch : 0 [33600/35700 (94%)]\tLoss: 0.097720\t Accuracy:96.560%\n",
      "Epoch : 0 [35200/35700 (99%)]\tLoss: 0.237786\t Accuracy:96.591%\n",
      "Epoch : 1 [0/35700 (0%)]\tLoss: 0.087088\t Accuracy:96.875%\n",
      "Epoch : 1 [1600/35700 (4%)]\tLoss: 0.182265\t Accuracy:98.591%\n",
      "Epoch : 1 [3200/35700 (9%)]\tLoss: 0.038985\t Accuracy:97.649%\n",
      "Epoch : 1 [4800/35700 (13%)]\tLoss: 0.034987\t Accuracy:97.661%\n",
      "Epoch : 1 [6400/35700 (18%)]\tLoss: 0.048147\t Accuracy:97.559%\n",
      "Epoch : 1 [8000/35700 (22%)]\tLoss: 0.301998\t Accuracy:97.560%\n",
      "Epoch : 1 [9600/35700 (27%)]\tLoss: 0.126440\t Accuracy:97.550%\n",
      "Epoch : 1 [11200/35700 (31%)]\tLoss: 0.000705\t Accuracy:97.427%\n",
      "Epoch : 1 [12800/35700 (36%)]\tLoss: 0.161488\t Accuracy:97.405%\n",
      "Epoch : 1 [14400/35700 (40%)]\tLoss: 0.073066\t Accuracy:97.395%\n",
      "Epoch : 1 [16000/35700 (45%)]\tLoss: 0.027369\t Accuracy:97.386%\n",
      "Epoch : 1 [17600/35700 (49%)]\tLoss: 0.069432\t Accuracy:97.249%\n",
      "Epoch : 1 [19200/35700 (54%)]\tLoss: 0.008987\t Accuracy:97.223%\n",
      "Epoch : 1 [20800/35700 (58%)]\tLoss: 0.036665\t Accuracy:97.177%\n",
      "Epoch : 1 [22400/35700 (63%)]\tLoss: 0.037761\t Accuracy:97.111%\n",
      "Epoch : 1 [24000/35700 (67%)]\tLoss: 0.144014\t Accuracy:97.066%\n",
      "Epoch : 1 [25600/35700 (72%)]\tLoss: 0.026402\t Accuracy:97.090%\n",
      "Epoch : 1 [27200/35700 (76%)]\tLoss: 0.326595\t Accuracy:97.136%\n",
      "Epoch : 1 [28800/35700 (81%)]\tLoss: 0.005784\t Accuracy:97.142%\n",
      "Epoch : 1 [30400/35700 (85%)]\tLoss: 0.042506\t Accuracy:97.138%\n",
      "Epoch : 1 [32000/35700 (90%)]\tLoss: 0.070987\t Accuracy:97.175%\n",
      "Epoch : 1 [33600/35700 (94%)]\tLoss: 0.257749\t Accuracy:97.149%\n",
      "Epoch : 1 [35200/35700 (99%)]\tLoss: 0.084845\t Accuracy:97.184%\n",
      "Epoch : 2 [0/35700 (0%)]\tLoss: 0.072627\t Accuracy:96.875%\n",
      "Epoch : 2 [1600/35700 (4%)]\tLoss: 0.349592\t Accuracy:97.181%\n",
      "Epoch : 2 [3200/35700 (9%)]\tLoss: 0.018832\t Accuracy:96.968%\n",
      "Epoch : 2 [4800/35700 (13%)]\tLoss: 0.041854\t Accuracy:97.165%\n",
      "Epoch : 2 [6400/35700 (18%)]\tLoss: 0.035932\t Accuracy:97.341%\n",
      "Epoch : 2 [8000/35700 (22%)]\tLoss: 0.404581\t Accuracy:97.385%\n",
      "Epoch : 2 [9600/35700 (27%)]\tLoss: 0.013022\t Accuracy:97.519%\n",
      "Epoch : 2 [11200/35700 (31%)]\tLoss: 0.019438\t Accuracy:97.525%\n",
      "Epoch : 2 [12800/35700 (36%)]\tLoss: 0.054312\t Accuracy:97.576%\n",
      "Epoch : 2 [14400/35700 (40%)]\tLoss: 0.033446\t Accuracy:97.609%\n",
      "Epoch : 2 [16000/35700 (45%)]\tLoss: 0.011664\t Accuracy:97.630%\n",
      "Epoch : 2 [17600/35700 (49%)]\tLoss: 0.008158\t Accuracy:97.635%\n",
      "Epoch : 2 [19200/35700 (54%)]\tLoss: 0.038354\t Accuracy:97.619%\n",
      "Epoch : 2 [20800/35700 (58%)]\tLoss: 0.267858\t Accuracy:97.595%\n",
      "Epoch : 2 [22400/35700 (63%)]\tLoss: 0.015698\t Accuracy:97.575%\n",
      "Epoch : 2 [24000/35700 (67%)]\tLoss: 0.004328\t Accuracy:97.599%\n",
      "Epoch : 2 [25600/35700 (72%)]\tLoss: 0.002009\t Accuracy:97.601%\n",
      "Epoch : 2 [27200/35700 (76%)]\tLoss: 0.265406\t Accuracy:97.617%\n",
      "Epoch : 2 [28800/35700 (81%)]\tLoss: 0.226262\t Accuracy:97.610%\n",
      "Epoch : 2 [30400/35700 (85%)]\tLoss: 0.092099\t Accuracy:97.608%\n",
      "Epoch : 2 [32000/35700 (90%)]\tLoss: 0.009351\t Accuracy:97.590%\n",
      "Epoch : 2 [33600/35700 (94%)]\tLoss: 0.045275\t Accuracy:97.589%\n",
      "Epoch : 2 [35200/35700 (99%)]\tLoss: 0.076736\t Accuracy:97.596%\n",
      "Epoch : 3 [0/35700 (0%)]\tLoss: 0.109990\t Accuracy:96.875%\n",
      "Epoch : 3 [1600/35700 (4%)]\tLoss: 0.100546\t Accuracy:97.733%\n",
      "Epoch : 3 [3200/35700 (9%)]\tLoss: 0.016166\t Accuracy:97.587%\n",
      "Epoch : 3 [4800/35700 (13%)]\tLoss: 0.069438\t Accuracy:97.434%\n",
      "Epoch : 3 [6400/35700 (18%)]\tLoss: 0.015534\t Accuracy:97.544%\n",
      "Epoch : 3 [8000/35700 (22%)]\tLoss: 0.443148\t Accuracy:97.597%\n",
      "Epoch : 3 [9600/35700 (27%)]\tLoss: 0.300222\t Accuracy:97.685%\n",
      "Epoch : 3 [11200/35700 (31%)]\tLoss: 0.009629\t Accuracy:97.667%\n",
      "Epoch : 3 [12800/35700 (36%)]\tLoss: 0.308195\t Accuracy:97.678%\n",
      "Epoch : 3 [14400/35700 (40%)]\tLoss: 0.071118\t Accuracy:97.713%\n",
      "Epoch : 3 [16000/35700 (45%)]\tLoss: 0.002238\t Accuracy:97.767%\n",
      "Epoch : 3 [17600/35700 (49%)]\tLoss: 0.003093\t Accuracy:97.680%\n",
      "Epoch : 3 [19200/35700 (54%)]\tLoss: 0.163442\t Accuracy:97.676%\n",
      "Epoch : 3 [20800/35700 (58%)]\tLoss: 0.163161\t Accuracy:97.686%\n",
      "Epoch : 3 [22400/35700 (63%)]\tLoss: 0.010118\t Accuracy:97.700%\n",
      "Epoch : 3 [24000/35700 (67%)]\tLoss: 0.016535\t Accuracy:97.657%\n",
      "Epoch : 3 [25600/35700 (72%)]\tLoss: 0.084521\t Accuracy:97.679%\n",
      "Epoch : 3 [27200/35700 (76%)]\tLoss: 0.009250\t Accuracy:97.683%\n",
      "Epoch : 3 [28800/35700 (81%)]\tLoss: 0.000014\t Accuracy:97.707%\n",
      "Epoch : 3 [30400/35700 (85%)]\tLoss: 0.200857\t Accuracy:97.670%\n",
      "Epoch : 3 [32000/35700 (90%)]\tLoss: 0.010141\t Accuracy:97.677%\n",
      "Epoch : 3 [33600/35700 (94%)]\tLoss: 0.108851\t Accuracy:97.639%\n",
      "Epoch : 3 [35200/35700 (99%)]\tLoss: 0.215227\t Accuracy:97.647%\n",
      "Epoch : 4 [0/35700 (0%)]\tLoss: 0.181545\t Accuracy:93.750%\n",
      "Epoch : 4 [1600/35700 (4%)]\tLoss: 0.344916\t Accuracy:97.978%\n",
      "Epoch : 4 [3200/35700 (9%)]\tLoss: 0.006106\t Accuracy:98.020%\n",
      "Epoch : 4 [4800/35700 (13%)]\tLoss: 0.080030\t Accuracy:97.889%\n",
      "Epoch : 4 [6400/35700 (18%)]\tLoss: 0.000996\t Accuracy:97.994%\n",
      "Epoch : 4 [8000/35700 (22%)]\tLoss: 0.508521\t Accuracy:98.120%\n",
      "Epoch : 4 [9600/35700 (27%)]\tLoss: 0.042873\t Accuracy:98.142%\n",
      "Epoch : 4 [11200/35700 (31%)]\tLoss: 0.051447\t Accuracy:98.032%\n",
      "Epoch : 4 [12800/35700 (36%)]\tLoss: 0.108267\t Accuracy:97.857%\n",
      "Epoch : 4 [14400/35700 (40%)]\tLoss: 0.001778\t Accuracy:97.838%\n",
      "Epoch : 4 [16000/35700 (45%)]\tLoss: 0.103550\t Accuracy:97.811%\n",
      "Epoch : 4 [17600/35700 (49%)]\tLoss: 0.005857\t Accuracy:97.788%\n",
      "Epoch : 4 [19200/35700 (54%)]\tLoss: 0.004997\t Accuracy:97.816%\n",
      "Epoch : 4 [20800/35700 (58%)]\tLoss: 0.017710\t Accuracy:97.825%\n",
      "Epoch : 4 [22400/35700 (63%)]\tLoss: 0.015394\t Accuracy:97.825%\n",
      "Epoch : 4 [24000/35700 (67%)]\tLoss: 0.002871\t Accuracy:97.840%\n",
      "Epoch : 4 [25600/35700 (72%)]\tLoss: 0.000861\t Accuracy:97.901%\n",
      "Epoch : 4 [27200/35700 (76%)]\tLoss: 0.002975\t Accuracy:97.896%\n",
      "Epoch : 4 [28800/35700 (81%)]\tLoss: 0.000022\t Accuracy:97.881%\n",
      "Epoch : 4 [30400/35700 (85%)]\tLoss: 0.056631\t Accuracy:97.881%\n",
      "Epoch : 4 [32000/35700 (90%)]\tLoss: 0.023813\t Accuracy:97.890%\n",
      "Epoch : 4 [33600/35700 (94%)]\tLoss: 0.453576\t Accuracy:97.838%\n",
      "Epoch : 4 [35200/35700 (99%)]\tLoss: 0.084915\t Accuracy:97.843%\n"
     ]
    }
   ],
   "source": [
    "fit(mlp, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.957% \n"
     ]
    }
   ],
   "source": [
    "def evaluate(model):\n",
    "#model = mlp\n",
    "    correct = 0 \n",
    "    for test_imgs, test_labels in test_loader:\n",
    "#         print(test_imgs.shape)\n",
    "        test_imgs = Variable(test_imgs).float()\n",
    "#         print(test_imgs.shape)\n",
    "        output = model(test_imgs)\n",
    "        predicted = torch.max(output,1)[1]\n",
    "        correct += (predicted == test_labels).sum()\n",
    "    print(\"Test accuracy:{:.3f}% \".format( float(correct) / (len(test_loader)*BATCH_SIZE)))\n",
    "evaluate(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4c54bdd87747d6946b71d8ee30afde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=100, description='im_index', max=300, min=-100), Output()), _dom_classesâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Sam Exploration\n",
    "# I want to run this model on an image of a digit and see, in near realtime, what it predicts\n",
    "from PIL import Image\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact # Interactive IPython is badass; Highly recommend. More info on setup here: https://ipywidgets.readthedocs.io/en/stable/examples/Using%20Interact.html\n",
    "\n",
    "SAMPLE_JPEGS = 'digit-recognizer/testSample/'\n",
    "SAMPLE_JPEGS = [SAMPLE_JPEGS +'/' +i for i in os.listdir(SAMPLE_JPEGS)]\n",
    "# im = Image.open(SAMPLE_JPEGS[0])\n",
    "# display(im)\n",
    "@interact\n",
    "def test_model(im_index = 100):\n",
    "    \"\"\"\n",
    "    Tests the Computer vision model in namespace as 'mlp', using an image read from the im_file filepath\n",
    "    \"\"\"\n",
    "    im_file = SAMPLE_JPEGS[im_index]\n",
    "    # Gets and displays the image \n",
    "    im = Image.open(im_file)\n",
    "    im_arr = np.array(im)\n",
    "    plt.imshow(im_arr, cmap ='binary')\n",
    "#     plt.show()\n",
    "    \n",
    "    # Ravel the array to get something to feed to the classifier neural net\n",
    "    im_arr = im_arr.ravel()\n",
    "    im_arr_test = torch.Tensor(im_arr).float()\n",
    "    im_arr_test = im_arr_test.unsqueeze(0) # Adds a dimension to the structure with size 1 at index 0; semantics\n",
    "#     print(im_arr_test.shape)\n",
    "    out = mlp(im_arr_test)\n",
    "    pred = torch.max(out,1)[1][0]\n",
    "    print('\\n\\n\\nThe model predicted {} for the below image\\n'.format(pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Convolution to the MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Define all of the layers in the __init__ method\n",
    "        # Conv2d is a class, so you are instantiating classes here? \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(32,64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(3*3*64, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define how the data will flow through the CNN\n",
    "        x = F.relu(self.conv1(x)) # Interesting how .conv1, defined in init, is a function\n",
    "        #x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(F.max_pool2d(self.conv3(x),2))\n",
    "        x = F.dropout(x, p=0.5, training=self.training) # I expect dropouts ~ pooling layers\n",
    "        # View? Are we looking at the\n",
    "        x = x.view(-1,3*3*64 )\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    " \n",
    "cnn = CNN()\n",
    "print(cnn)\n",
    "\n",
    "it = iter(train_loader)\n",
    "X_batch, y_batch = next(it)\n",
    "# print(cnn.forward(X_batch).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight 32 1, but got 2-dimensional input of size [32, 784] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-1c26c3cd6f27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-88-0d7eaaa7cff0>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, train_loader)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mvar_y_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_X_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_y_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Computer_Vision/env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-98-704a80590559>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Define how the data will flow through the CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Interesting how .conv1, defined in init, is a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m#x = F.dropout(x, p=0.5, training=self.training)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Computer_Vision/env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Computer_Vision/env/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    336\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    337\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 338\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight 32 1, but got 2-dimensional input of size [32, 784] instead"
     ]
    }
   ],
   "source": [
    "fit(cnn,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (linear1): Linear(in_features=784, out_features=250, bias=True)\n",
       "  (linear2): Linear(in_features=250, out_features=100, bias=True)\n",
       "  (linear3): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# help(mlp.eval())\n",
    "mlp.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "cv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
