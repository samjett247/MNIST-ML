{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working from this [kernel in Kaggle](https://www.kaggle.com/sdelecourt/cnn-with-pytorch-for-mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # to handle matrix and data operation\n",
    "import pandas as pd # to read csv and handle dataframe\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to keep training parameters in the same place\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('digit-recognizer/train.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label'].values\n",
    "X = df.drop(['label'],1).values # Drop the labels so you don't get data pollution\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15) # Split into test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_X_train = torch.from_numpy(X_train).type(torch.LongTensor)\n",
    "torch_y_train = torch.from_numpy(y_train).type(torch.LongTensor) # data type is long; But why?\n",
    "\n",
    "# create feature and targets tensor for test set\n",
    "torch_X_test = torch.from_numpy(X_test).type(torch.LongTensor)\n",
    "torch_y_test = torch.from_numpy(y_test).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\n",
    "test = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the data flow in torch in this example goes follows:\n",
    "1. Read the data in from csv file\n",
    "2. Convert the data to numpy array (can be combined with step 1)\n",
    "3. Split the dataset into test and train data, using scikit learn functionality\n",
    "4. Convert the test and train datasets into *Torch Tensor*\n",
    "5. Convert the *Torch Tensors* into *Tensor Datasets* for both train and test data\n",
    "6. Input the Datasets into a data loader, considering a specified batch size\n",
    "\n",
    "Seems like an unnecessary number of steps; Chance to optimize, at the very least for shorter and clearer code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear1): Linear(in_features=784, out_features=250, bias=True)\n",
      "  (linear2): Linear(in_features=250, out_features=100, bias=True)\n",
      "  (linear3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Same as other example; Create an inherited class for the neural network\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(784,250)\n",
    "        self.linear2 = nn.Linear(250,100)\n",
    "        self.linear3 = nn.Linear(100,10)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        return F.log_softmax(X, dim=1)\n",
    " \n",
    "mlp = MLP()\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'*' Copied from the example code\n",
    "\n",
    "We have 784*(250+1) + 250*(100+1) + 100*(10+1) = 222 360 parameters to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader):\n",
    "    # Adam is a method for stochastic gradient descent; avail. below\n",
    "    # https://pytorch.org/docs/stable/_modules/torch/optim/adam.html\n",
    "    optimizer = torch.optim.Adam(model.parameters())#,lr=0.001, betas=(0.9,0.999))\n",
    "    error = nn.CrossEntropyLoss() # Cross Entropy loss is a loss fxn, AKA log loss\n",
    "    # Read more aboout cross entropy, or log loss, here: https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html\n",
    "    model.train() # Calling .train() method on nn.Module object\n",
    "    for epoch in range(EPOCHS):\n",
    "        correct = 0\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(var_X_batch)\n",
    "            loss = error(output, var_y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Total correct predictions\n",
    "            predicted = torch.max(output.data, 1)[1] \n",
    "            correct += (predicted == var_y_batch).sum()\n",
    "            #print(correct)\n",
    "            if batch_idx % 50 == 0:\n",
    "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.item(), float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/35700 (0%)]\tLoss: 0.245441\t Accuracy:93.750%\n",
      "Epoch : 0 [1600/35700 (4%)]\tLoss: 0.050921\t Accuracy:98.100%\n",
      "Epoch : 0 [3200/35700 (9%)]\tLoss: 0.168699\t Accuracy:98.577%\n",
      "Epoch : 0 [4800/35700 (13%)]\tLoss: 0.017594\t Accuracy:98.717%\n",
      "Epoch : 0 [6400/35700 (18%)]\tLoss: 0.259139\t Accuracy:98.678%\n",
      "Epoch : 0 [8000/35700 (22%)]\tLoss: 0.147993\t Accuracy:98.518%\n",
      "Epoch : 0 [9600/35700 (27%)]\tLoss: 0.017814\t Accuracy:98.422%\n",
      "Epoch : 0 [11200/35700 (31%)]\tLoss: 0.101588\t Accuracy:98.317%\n",
      "Epoch : 0 [12800/35700 (36%)]\tLoss: 0.000706\t Accuracy:98.286%\n",
      "Epoch : 0 [14400/35700 (40%)]\tLoss: 0.318679\t Accuracy:98.101%\n",
      "Epoch : 0 [16000/35700 (45%)]\tLoss: 0.759212\t Accuracy:98.029%\n",
      "Epoch : 0 [17600/35700 (49%)]\tLoss: 0.000487\t Accuracy:97.970%\n",
      "Epoch : 0 [19200/35700 (54%)]\tLoss: 0.291597\t Accuracy:97.972%\n",
      "Epoch : 0 [20800/35700 (58%)]\tLoss: 0.012098\t Accuracy:98.017%\n",
      "Epoch : 0 [22400/35700 (63%)]\tLoss: 0.000094\t Accuracy:98.052%\n",
      "Epoch : 0 [24000/35700 (67%)]\tLoss: 0.003601\t Accuracy:98.065%\n",
      "Epoch : 0 [25600/35700 (72%)]\tLoss: 0.001304\t Accuracy:98.022%\n",
      "Epoch : 0 [27200/35700 (76%)]\tLoss: 0.000205\t Accuracy:98.032%\n",
      "Epoch : 0 [28800/35700 (81%)]\tLoss: 0.019975\t Accuracy:98.047%\n",
      "Epoch : 0 [30400/35700 (85%)]\tLoss: 0.046095\t Accuracy:98.038%\n",
      "Epoch : 0 [32000/35700 (90%)]\tLoss: 0.000905\t Accuracy:98.058%\n",
      "Epoch : 0 [33600/35700 (94%)]\tLoss: 0.000632\t Accuracy:98.061%\n",
      "Epoch : 0 [35200/35700 (99%)]\tLoss: 0.355953\t Accuracy:98.070%\n",
      "Epoch : 1 [0/35700 (0%)]\tLoss: 0.175465\t Accuracy:96.875%\n",
      "Epoch : 1 [1600/35700 (4%)]\tLoss: 0.011175\t Accuracy:97.855%\n",
      "Epoch : 1 [3200/35700 (9%)]\tLoss: 0.025908\t Accuracy:98.298%\n",
      "Epoch : 1 [4800/35700 (13%)]\tLoss: 0.033922\t Accuracy:98.200%\n",
      "Epoch : 1 [6400/35700 (18%)]\tLoss: 0.000200\t Accuracy:98.321%\n",
      "Epoch : 1 [8000/35700 (22%)]\tLoss: 0.091403\t Accuracy:98.282%\n",
      "Epoch : 1 [9600/35700 (27%)]\tLoss: 0.089793\t Accuracy:98.370%\n",
      "Epoch : 1 [11200/35700 (31%)]\tLoss: 0.053786\t Accuracy:98.317%\n",
      "Epoch : 1 [12800/35700 (36%)]\tLoss: 0.001076\t Accuracy:98.231%\n",
      "Epoch : 1 [14400/35700 (40%)]\tLoss: 0.019050\t Accuracy:98.289%\n",
      "Epoch : 1 [16000/35700 (45%)]\tLoss: 0.000039\t Accuracy:98.316%\n",
      "Epoch : 1 [17600/35700 (49%)]\tLoss: 0.000054\t Accuracy:98.344%\n",
      "Epoch : 1 [19200/35700 (54%)]\tLoss: 0.000033\t Accuracy:98.352%\n",
      "Epoch : 1 [20800/35700 (58%)]\tLoss: 0.000385\t Accuracy:98.325%\n",
      "Epoch : 1 [22400/35700 (63%)]\tLoss: 0.000141\t Accuracy:98.333%\n",
      "Epoch : 1 [24000/35700 (67%)]\tLoss: 0.000893\t Accuracy:98.377%\n",
      "Epoch : 1 [25600/35700 (72%)]\tLoss: 0.661229\t Accuracy:98.373%\n",
      "Epoch : 1 [27200/35700 (76%)]\tLoss: 0.002492\t Accuracy:98.395%\n",
      "Epoch : 1 [28800/35700 (81%)]\tLoss: 0.000052\t Accuracy:98.418%\n",
      "Epoch : 1 [30400/35700 (85%)]\tLoss: 0.015392\t Accuracy:98.436%\n",
      "Epoch : 1 [32000/35700 (90%)]\tLoss: 0.037169\t Accuracy:98.430%\n",
      "Epoch : 1 [33600/35700 (94%)]\tLoss: 0.013852\t Accuracy:98.415%\n",
      "Epoch : 1 [35200/35700 (99%)]\tLoss: 0.409904\t Accuracy:98.402%\n",
      "Epoch : 2 [0/35700 (0%)]\tLoss: 0.397998\t Accuracy:93.750%\n",
      "Epoch : 2 [1600/35700 (4%)]\tLoss: 0.002877\t Accuracy:97.488%\n",
      "Epoch : 2 [3200/35700 (9%)]\tLoss: 0.267413\t Accuracy:98.051%\n",
      "Epoch : 2 [4800/35700 (13%)]\tLoss: 0.001825\t Accuracy:98.096%\n",
      "Epoch : 2 [6400/35700 (18%)]\tLoss: 0.000247\t Accuracy:98.212%\n",
      "Epoch : 2 [8000/35700 (22%)]\tLoss: 0.088674\t Accuracy:98.232%\n",
      "Epoch : 2 [9600/35700 (27%)]\tLoss: 0.000526\t Accuracy:98.370%\n",
      "Epoch : 2 [11200/35700 (31%)]\tLoss: 0.006625\t Accuracy:98.469%\n",
      "Epoch : 2 [12800/35700 (36%)]\tLoss: 0.000331\t Accuracy:98.519%\n",
      "Epoch : 2 [14400/35700 (40%)]\tLoss: 0.090720\t Accuracy:98.566%\n",
      "Epoch : 2 [16000/35700 (45%)]\tLoss: 0.000081\t Accuracy:98.584%\n",
      "Epoch : 2 [17600/35700 (49%)]\tLoss: 0.000225\t Accuracy:98.582%\n",
      "Epoch : 2 [19200/35700 (54%)]\tLoss: 0.009988\t Accuracy:98.560%\n",
      "Epoch : 2 [20800/35700 (58%)]\tLoss: 0.000020\t Accuracy:98.526%\n",
      "Epoch : 2 [22400/35700 (63%)]\tLoss: 0.016940\t Accuracy:98.520%\n",
      "Epoch : 2 [24000/35700 (67%)]\tLoss: 0.000340\t Accuracy:98.556%\n",
      "Epoch : 2 [25600/35700 (72%)]\tLoss: 0.016849\t Accuracy:98.572%\n",
      "Epoch : 2 [27200/35700 (76%)]\tLoss: 0.001697\t Accuracy:98.594%\n",
      "Epoch : 2 [28800/35700 (81%)]\tLoss: 0.002518\t Accuracy:98.602%\n",
      "Epoch : 2 [30400/35700 (85%)]\tLoss: 0.014024\t Accuracy:98.584%\n",
      "Epoch : 2 [32000/35700 (90%)]\tLoss: 0.008678\t Accuracy:98.598%\n",
      "Epoch : 2 [33600/35700 (94%)]\tLoss: 0.000608\t Accuracy:98.620%\n",
      "Epoch : 2 [35200/35700 (99%)]\tLoss: 0.089111\t Accuracy:98.615%\n",
      "Epoch : 3 [0/35700 (0%)]\tLoss: 0.147572\t Accuracy:93.750%\n",
      "Epoch : 3 [1600/35700 (4%)]\tLoss: 0.000597\t Accuracy:97.794%\n",
      "Epoch : 3 [3200/35700 (9%)]\tLoss: 0.001905\t Accuracy:97.741%\n",
      "Epoch : 3 [4800/35700 (13%)]\tLoss: 0.000423\t Accuracy:98.055%\n",
      "Epoch : 3 [6400/35700 (18%)]\tLoss: 0.004690\t Accuracy:98.150%\n",
      "Epoch : 3 [8000/35700 (22%)]\tLoss: 0.001727\t Accuracy:98.108%\n",
      "Epoch : 3 [9600/35700 (27%)]\tLoss: 0.000674\t Accuracy:98.079%\n",
      "Epoch : 3 [11200/35700 (31%)]\tLoss: 0.058869\t Accuracy:98.077%\n",
      "Epoch : 3 [12800/35700 (36%)]\tLoss: 0.008383\t Accuracy:98.091%\n",
      "Epoch : 3 [14400/35700 (40%)]\tLoss: 0.021739\t Accuracy:98.101%\n",
      "Epoch : 3 [16000/35700 (45%)]\tLoss: 0.000180\t Accuracy:98.104%\n",
      "Epoch : 3 [17600/35700 (49%)]\tLoss: 0.000024\t Accuracy:98.100%\n",
      "Epoch : 3 [19200/35700 (54%)]\tLoss: 0.001593\t Accuracy:98.133%\n",
      "Epoch : 3 [20800/35700 (58%)]\tLoss: 0.001912\t Accuracy:98.161%\n",
      "Epoch : 3 [22400/35700 (63%)]\tLoss: 0.005089\t Accuracy:98.186%\n",
      "Epoch : 3 [24000/35700 (67%)]\tLoss: 0.000000\t Accuracy:98.215%\n",
      "Epoch : 3 [25600/35700 (72%)]\tLoss: 0.011994\t Accuracy:98.225%\n",
      "Epoch : 3 [27200/35700 (76%)]\tLoss: 0.036863\t Accuracy:98.234%\n",
      "Epoch : 3 [28800/35700 (81%)]\tLoss: 0.004219\t Accuracy:98.242%\n",
      "Epoch : 3 [30400/35700 (85%)]\tLoss: 0.059642\t Accuracy:98.235%\n",
      "Epoch : 3 [32000/35700 (90%)]\tLoss: 0.000528\t Accuracy:98.249%\n",
      "Epoch : 3 [33600/35700 (94%)]\tLoss: 0.001606\t Accuracy:98.261%\n",
      "Epoch : 3 [35200/35700 (99%)]\tLoss: 0.626446\t Accuracy:98.249%\n",
      "Epoch : 4 [0/35700 (0%)]\tLoss: 0.179238\t Accuracy:93.750%\n",
      "Epoch : 4 [1600/35700 (4%)]\tLoss: 0.002490\t Accuracy:98.223%\n",
      "Epoch : 4 [3200/35700 (9%)]\tLoss: 0.013210\t Accuracy:98.205%\n",
      "Epoch : 4 [4800/35700 (13%)]\tLoss: 0.001567\t Accuracy:98.200%\n",
      "Epoch : 4 [6400/35700 (18%)]\tLoss: 0.003337\t Accuracy:98.290%\n",
      "Epoch : 4 [8000/35700 (22%)]\tLoss: 0.036244\t Accuracy:98.332%\n",
      "Epoch : 4 [9600/35700 (27%)]\tLoss: 0.003580\t Accuracy:98.463%\n",
      "Epoch : 4 [11200/35700 (31%)]\tLoss: 0.000954\t Accuracy:98.442%\n",
      "Epoch : 4 [12800/35700 (36%)]\tLoss: 0.009652\t Accuracy:98.480%\n",
      "Epoch : 4 [14400/35700 (40%)]\tLoss: 0.061051\t Accuracy:98.476%\n",
      "Epoch : 4 [16000/35700 (45%)]\tLoss: 0.000662\t Accuracy:98.472%\n",
      "Epoch : 4 [17600/35700 (49%)]\tLoss: 0.033646\t Accuracy:98.457%\n",
      "Epoch : 4 [19200/35700 (54%)]\tLoss: 0.005316\t Accuracy:98.430%\n",
      "Epoch : 4 [20800/35700 (58%)]\tLoss: 0.015739\t Accuracy:98.411%\n",
      "Epoch : 4 [22400/35700 (63%)]\tLoss: 0.000889\t Accuracy:98.431%\n",
      "Epoch : 4 [24000/35700 (67%)]\tLoss: 0.000053\t Accuracy:98.427%\n",
      "Epoch : 4 [25600/35700 (72%)]\tLoss: 0.028225\t Accuracy:98.428%\n",
      "Epoch : 4 [27200/35700 (76%)]\tLoss: 0.004631\t Accuracy:98.421%\n",
      "Epoch : 4 [28800/35700 (81%)]\tLoss: 0.000046\t Accuracy:98.432%\n",
      "Epoch : 4 [30400/35700 (85%)]\tLoss: 0.048008\t Accuracy:98.436%\n",
      "Epoch : 4 [32000/35700 (90%)]\tLoss: 0.000130\t Accuracy:98.455%\n",
      "Epoch : 4 [33600/35700 (94%)]\tLoss: 0.005559\t Accuracy:98.418%\n",
      "Epoch : 4 [35200/35700 (99%)]\tLoss: 0.148019\t Accuracy:98.402%\n"
     ]
    }
   ],
   "source": [
    "fit(mlp, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.962% \n"
     ]
    }
   ],
   "source": [
    "def evaluate(model):\n",
    "#model = mlp\n",
    "    correct = 0 \n",
    "    for test_imgs, test_labels in test_loader:\n",
    "#         print(test_imgs.shape)\n",
    "        test_imgs = Variable(test_imgs).float()\n",
    "#         print(test_imgs.shape)\n",
    "        output = model(test_imgs)\n",
    "        predicted = torch.max(output,1)[1]\n",
    "        correct += (predicted == test_labels).sum()\n",
    "    print(\"Test accuracy:{:.3f}% \".format( float(correct) / (len(test_loader)*BATCH_SIZE)))\n",
    "evaluate(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa310594641479198a7a9cb952ee02a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=100, description='im_index', max=300, min=-100), Output()), _dom_classes…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Sam Exploration\n",
    "# I want to run this model on an image of a digit and see, in near realtime, what it predicts\n",
    "from PIL import Image\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact # Interactive IPython is badass; Highly recommend. More info on setup here: https://ipywidgets.readthedocs.io/en/stable/examples/Using%20Interact.html\n",
    "\n",
    "\n",
    "SAMPLE_JPEGS = 'digit-recognizer/testSample/'\n",
    "SAMPLE_JPEGS = [SAMPLE_JPEGS +'/' +i for i in os.listdir(SAMPLE_JPEGS)]\n",
    "# im = Image.open(SAMPLE_JPEGS[0])\n",
    "# display(im)\n",
    "@interact\n",
    "def test_model(im_index = 100):\n",
    "    \"\"\"\n",
    "    Tests the Computer vision model in namespace as 'mlp', using an image read from the im_file filepath\n",
    "    \"\"\"\n",
    "    im_file = SAMPLE_JPEGS[im_index]\n",
    "    # Gets and displays the image \n",
    "    im = Image.open(im_file)\n",
    "    im_arr = np.array(im)\n",
    "    plt.imshow(im_arr, cmap ='binary')\n",
    "#     plt.show()\n",
    "    \n",
    "    # Ravel the array to get something to feed to the classifier neural net\n",
    "    im_arr = im_arr.ravel()\n",
    "    im_arr_test = torch.Tensor(im_arr).float()\n",
    "    im_arr_test = im_arr_test.unsqueeze(0) # Adds a dimension to the structure with size 1 at index 0; semantics\n",
    "#     print(im_arr_test.shape)\n",
    "    out = mlp(im_arr_test)\n",
    "    pred = torch.max(out,1)[1][0]\n",
    "    print('\\n\\n\\nThe model predicted {} for the below image'.format(pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (linear1): Linear(in_features=784, out_features=250, bias=True)\n",
       "  (linear2): Linear(in_features=250, out_features=100, bias=True)\n",
       "  (linear3): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# help(mlp.eval())\n",
    "mlp.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "cv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
