{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working from this [kernel in Kaggle](https://www.kaggle.com/sdelecourt/cnn-with-pytorch-for-mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # to handle matrix and data operation\n",
    "import pandas as pd # to read csv and handle dataframe\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to keep training parameters in the same place\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('digit-recognizer/train.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label'].values\n",
    "X = df.drop(['label'],1).values # Drop the labels so you don't get data pollution\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15) # Split into test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_X_train = torch.from_numpy(X_train).type(torch.LongTensor)\n",
    "torch_y_train = torch.from_numpy(y_train).type(torch.LongTensor) # data type is long; But why?\n",
    "\n",
    "# create feature and targets tensor for test set\n",
    "torch_X_test = torch.from_numpy(X_test).type(torch.LongTensor)\n",
    "torch_y_test = torch.from_numpy(y_test).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\n",
    "test = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the data flow in torch in this example goes follows:\n",
    "1. Read the data in from csv file\n",
    "2. Convert the data to numpy array (can be combined with step 1)\n",
    "3. Split the dataset into test and train data, using scikit learn functionality\n",
    "4. Convert the test and train datasets into *Torch Tensor*\n",
    "5. Convert the *Torch Tensors* into *Tensor Datasets* for both train and test data\n",
    "6. Input the Datasets into a data loader, considering a specified batch size\n",
    "\n",
    "Seems like an unnecessary number of steps; Chance to optimize, at the very least for shorter and clearer code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear1): Linear(in_features=784, out_features=250, bias=True)\n",
      "  (linear2): Linear(in_features=250, out_features=100, bias=True)\n",
      "  (linear3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Same as other example; Create an inherited class for the neural network\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(784,250)\n",
    "        self.linear2 = nn.Linear(250,100)\n",
    "        self.linear3 = nn.Linear(100,10)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        return F.log_softmax(X, dim=1)\n",
    " \n",
    "mlp = MLP()\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'*' Copied from the example code\n",
    "\n",
    "We have 784*(250+1) + 250*(100+1) + 100*(10+1) = 222 360 parameters to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader):\n",
    "    # Adam is a method for stochastic gradient descent; avail. below\n",
    "    # https://pytorch.org/docs/stable/_modules/torch/optim/adam.html\n",
    "    optimizer = torch.optim.Adam(model.parameters())#,lr=0.001, betas=(0.9,0.999))\n",
    "    error = nn.CrossEntropyLoss() # Cross Entropy loss is a loss fxn, AKA log loss\n",
    "    # Read more aboout cross entropy, or log loss, here: https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html\n",
    "    model.train() # Calling .train() method on nn.Module object\n",
    "    for epoch in range(EPOCHS):\n",
    "        correct = 0\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(var_X_batch)\n",
    "            loss = error(output, var_y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Total correct predictions\n",
    "            predicted = torch.max(output.data, 1)[1] \n",
    "            correct += (predicted == var_y_batch).sum()\n",
    "            #print(correct)\n",
    "            if batch_idx % 50 == 0:\n",
    "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.item(), float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/35700 (0%)]\tLoss: 0.408600\t Accuracy:96.875%\n",
      "Epoch : 0 [1600/35700 (4%)]\tLoss: 0.005384\t Accuracy:98.468%\n",
      "Epoch : 0 [3200/35700 (9%)]\tLoss: 0.007809\t Accuracy:98.360%\n",
      "Epoch : 0 [4800/35700 (13%)]\tLoss: 0.011668\t Accuracy:98.489%\n",
      "Epoch : 0 [6400/35700 (18%)]\tLoss: 0.001402\t Accuracy:98.243%\n",
      "Epoch : 0 [8000/35700 (22%)]\tLoss: 0.048860\t Accuracy:98.095%\n",
      "Epoch : 0 [9600/35700 (27%)]\tLoss: 0.158247\t Accuracy:98.100%\n",
      "Epoch : 0 [11200/35700 (31%)]\tLoss: 0.060775\t Accuracy:98.015%\n",
      "Epoch : 0 [12800/35700 (36%)]\tLoss: 0.024067\t Accuracy:97.927%\n",
      "Epoch : 0 [14400/35700 (40%)]\tLoss: 0.252585\t Accuracy:97.894%\n",
      "Epoch : 0 [16000/35700 (45%)]\tLoss: 0.000414\t Accuracy:97.917%\n",
      "Epoch : 0 [17600/35700 (49%)]\tLoss: 0.000932\t Accuracy:97.902%\n",
      "Epoch : 0 [19200/35700 (54%)]\tLoss: 0.172058\t Accuracy:97.889%\n",
      "Epoch : 0 [20800/35700 (58%)]\tLoss: 0.089318\t Accuracy:97.883%\n",
      "Epoch : 0 [22400/35700 (63%)]\tLoss: 0.000671\t Accuracy:97.838%\n",
      "Epoch : 0 [24000/35700 (67%)]\tLoss: 0.001018\t Accuracy:97.828%\n",
      "Epoch : 0 [25600/35700 (72%)]\tLoss: 0.012332\t Accuracy:97.835%\n",
      "Epoch : 0 [27200/35700 (76%)]\tLoss: 0.006237\t Accuracy:97.852%\n",
      "Epoch : 0 [28800/35700 (81%)]\tLoss: 0.106889\t Accuracy:97.867%\n",
      "Epoch : 0 [30400/35700 (85%)]\tLoss: 0.063473\t Accuracy:97.887%\n",
      "Epoch : 0 [32000/35700 (90%)]\tLoss: 0.000784\t Accuracy:97.871%\n",
      "Epoch : 0 [33600/35700 (94%)]\tLoss: 0.000007\t Accuracy:97.871%\n",
      "Epoch : 0 [35200/35700 (99%)]\tLoss: 0.536681\t Accuracy:97.868%\n",
      "Epoch : 1 [0/35700 (0%)]\tLoss: 0.271595\t Accuracy:96.875%\n",
      "Epoch : 1 [1600/35700 (4%)]\tLoss: 0.000984\t Accuracy:97.794%\n",
      "Epoch : 1 [3200/35700 (9%)]\tLoss: 0.170540\t Accuracy:97.803%\n",
      "Epoch : 1 [4800/35700 (13%)]\tLoss: 0.026259\t Accuracy:97.868%\n",
      "Epoch : 1 [6400/35700 (18%)]\tLoss: 0.001433\t Accuracy:97.854%\n",
      "Epoch : 1 [8000/35700 (22%)]\tLoss: 0.022845\t Accuracy:97.709%\n",
      "Epoch : 1 [9600/35700 (27%)]\tLoss: 0.000296\t Accuracy:97.726%\n",
      "Epoch : 1 [11200/35700 (31%)]\tLoss: 0.438976\t Accuracy:97.534%\n",
      "Epoch : 1 [12800/35700 (36%)]\tLoss: 0.004767\t Accuracy:97.584%\n",
      "Epoch : 1 [14400/35700 (40%)]\tLoss: 0.085534\t Accuracy:97.665%\n",
      "Epoch : 1 [16000/35700 (45%)]\tLoss: 0.000666\t Accuracy:97.692%\n",
      "Epoch : 1 [17600/35700 (49%)]\tLoss: 0.004456\t Accuracy:97.635%\n",
      "Epoch : 1 [19200/35700 (54%)]\tLoss: 0.040412\t Accuracy:97.671%\n",
      "Epoch : 1 [20800/35700 (58%)]\tLoss: 0.006511\t Accuracy:97.681%\n",
      "Epoch : 1 [22400/35700 (63%)]\tLoss: 0.000040\t Accuracy:97.704%\n",
      "Epoch : 1 [24000/35700 (67%)]\tLoss: 0.001016\t Accuracy:97.686%\n",
      "Epoch : 1 [25600/35700 (72%)]\tLoss: 0.000342\t Accuracy:97.690%\n",
      "Epoch : 1 [27200/35700 (76%)]\tLoss: 0.001663\t Accuracy:97.742%\n",
      "Epoch : 1 [28800/35700 (81%)]\tLoss: 0.000476\t Accuracy:97.777%\n",
      "Epoch : 1 [30400/35700 (85%)]\tLoss: 0.144463\t Accuracy:97.802%\n",
      "Epoch : 1 [32000/35700 (90%)]\tLoss: 0.002349\t Accuracy:97.821%\n",
      "Epoch : 1 [33600/35700 (94%)]\tLoss: 0.043129\t Accuracy:97.838%\n",
      "Epoch : 1 [35200/35700 (99%)]\tLoss: 0.373178\t Accuracy:97.829%\n",
      "Epoch : 2 [0/35700 (0%)]\tLoss: 0.053174\t Accuracy:100.000%\n",
      "Epoch : 2 [1600/35700 (4%)]\tLoss: 0.008528\t Accuracy:97.978%\n",
      "Epoch : 2 [3200/35700 (9%)]\tLoss: 0.023763\t Accuracy:98.453%\n",
      "Epoch : 2 [4800/35700 (13%)]\tLoss: 0.000549\t Accuracy:98.469%\n",
      "Epoch : 2 [6400/35700 (18%)]\tLoss: 0.124654\t Accuracy:98.445%\n",
      "Epoch : 2 [8000/35700 (22%)]\tLoss: 0.028828\t Accuracy:98.269%\n",
      "Epoch : 2 [9600/35700 (27%)]\tLoss: 0.001981\t Accuracy:98.225%\n",
      "Epoch : 2 [11200/35700 (31%)]\tLoss: 0.052315\t Accuracy:98.024%\n",
      "Epoch : 2 [12800/35700 (36%)]\tLoss: 0.045858\t Accuracy:98.028%\n",
      "Epoch : 2 [14400/35700 (40%)]\tLoss: 0.037788\t Accuracy:98.032%\n",
      "Epoch : 2 [16000/35700 (45%)]\tLoss: 0.000097\t Accuracy:98.035%\n",
      "Epoch : 2 [17600/35700 (49%)]\tLoss: 0.039203\t Accuracy:98.026%\n",
      "Epoch : 2 [19200/35700 (54%)]\tLoss: 0.003826\t Accuracy:98.061%\n",
      "Epoch : 2 [20800/35700 (58%)]\tLoss: 0.416201\t Accuracy:98.094%\n",
      "Epoch : 2 [22400/35700 (63%)]\tLoss: 0.000461\t Accuracy:98.105%\n",
      "Epoch : 2 [24000/35700 (67%)]\tLoss: 0.067454\t Accuracy:98.090%\n",
      "Epoch : 2 [25600/35700 (72%)]\tLoss: 0.001883\t Accuracy:98.081%\n",
      "Epoch : 2 [27200/35700 (76%)]\tLoss: 0.006028\t Accuracy:98.098%\n",
      "Epoch : 2 [28800/35700 (81%)]\tLoss: 0.002541\t Accuracy:98.158%\n",
      "Epoch : 2 [30400/35700 (85%)]\tLoss: 0.061000\t Accuracy:98.173%\n",
      "Epoch : 2 [32000/35700 (90%)]\tLoss: 0.031465\t Accuracy:98.171%\n",
      "Epoch : 2 [33600/35700 (94%)]\tLoss: 0.000484\t Accuracy:98.207%\n",
      "Epoch : 2 [35200/35700 (99%)]\tLoss: 0.334649\t Accuracy:98.198%\n",
      "Epoch : 3 [0/35700 (0%)]\tLoss: 0.144941\t Accuracy:96.875%\n",
      "Epoch : 3 [1600/35700 (4%)]\tLoss: 0.007353\t Accuracy:98.162%\n",
      "Epoch : 3 [3200/35700 (9%)]\tLoss: 0.074378\t Accuracy:98.236%\n",
      "Epoch : 3 [4800/35700 (13%)]\tLoss: 0.001716\t Accuracy:98.386%\n",
      "Epoch : 3 [6400/35700 (18%)]\tLoss: 0.002979\t Accuracy:98.321%\n",
      "Epoch : 3 [8000/35700 (22%)]\tLoss: 0.032547\t Accuracy:98.319%\n",
      "Epoch : 3 [9600/35700 (27%)]\tLoss: 0.006735\t Accuracy:98.245%\n",
      "Epoch : 3 [11200/35700 (31%)]\tLoss: 0.092058\t Accuracy:98.228%\n",
      "Epoch : 3 [12800/35700 (36%)]\tLoss: 0.037273\t Accuracy:98.161%\n",
      "Epoch : 3 [14400/35700 (40%)]\tLoss: 0.315778\t Accuracy:98.088%\n",
      "Epoch : 3 [16000/35700 (45%)]\tLoss: 0.001764\t Accuracy:98.091%\n",
      "Epoch : 3 [17600/35700 (49%)]\tLoss: 0.000324\t Accuracy:98.077%\n",
      "Epoch : 3 [19200/35700 (54%)]\tLoss: 0.028713\t Accuracy:98.097%\n",
      "Epoch : 3 [20800/35700 (58%)]\tLoss: 0.001807\t Accuracy:98.094%\n",
      "Epoch : 3 [22400/35700 (63%)]\tLoss: 0.041821\t Accuracy:98.110%\n",
      "Epoch : 3 [24000/35700 (67%)]\tLoss: 0.000174\t Accuracy:98.115%\n",
      "Epoch : 3 [25600/35700 (72%)]\tLoss: 0.014770\t Accuracy:98.143%\n",
      "Epoch : 3 [27200/35700 (76%)]\tLoss: 0.001384\t Accuracy:98.168%\n",
      "Epoch : 3 [28800/35700 (81%)]\tLoss: 0.014616\t Accuracy:98.183%\n",
      "Epoch : 3 [30400/35700 (85%)]\tLoss: 0.054877\t Accuracy:98.196%\n",
      "Epoch : 3 [32000/35700 (90%)]\tLoss: 0.001849\t Accuracy:98.164%\n",
      "Epoch : 3 [33600/35700 (94%)]\tLoss: 0.003027\t Accuracy:98.124%\n",
      "Epoch : 3 [35200/35700 (99%)]\tLoss: 0.607649\t Accuracy:98.093%\n",
      "Epoch : 4 [0/35700 (0%)]\tLoss: 0.264547\t Accuracy:93.750%\n",
      "Epoch : 4 [1600/35700 (4%)]\tLoss: 0.094553\t Accuracy:98.223%\n",
      "Epoch : 4 [3200/35700 (9%)]\tLoss: 0.145252\t Accuracy:98.236%\n",
      "Epoch : 4 [4800/35700 (13%)]\tLoss: 0.001618\t Accuracy:98.386%\n",
      "Epoch : 4 [6400/35700 (18%)]\tLoss: 0.000406\t Accuracy:98.228%\n",
      "Epoch : 4 [8000/35700 (22%)]\tLoss: 0.033173\t Accuracy:98.182%\n",
      "Epoch : 4 [9600/35700 (27%)]\tLoss: 0.061900\t Accuracy:98.277%\n",
      "Epoch : 4 [11200/35700 (31%)]\tLoss: 0.029492\t Accuracy:98.246%\n",
      "Epoch : 4 [12800/35700 (36%)]\tLoss: 0.112969\t Accuracy:98.223%\n",
      "Epoch : 4 [14400/35700 (40%)]\tLoss: 0.049034\t Accuracy:98.233%\n",
      "Epoch : 4 [16000/35700 (45%)]\tLoss: 0.006891\t Accuracy:98.216%\n",
      "Epoch : 4 [17600/35700 (49%)]\tLoss: 0.033734\t Accuracy:98.196%\n",
      "Epoch : 4 [19200/35700 (54%)]\tLoss: 0.000095\t Accuracy:98.243%\n",
      "Epoch : 4 [20800/35700 (58%)]\tLoss: 0.002152\t Accuracy:98.267%\n",
      "Epoch : 4 [22400/35700 (63%)]\tLoss: 0.227320\t Accuracy:98.270%\n",
      "Epoch : 4 [24000/35700 (67%)]\tLoss: 0.001771\t Accuracy:98.281%\n",
      "Epoch : 4 [25600/35700 (72%)]\tLoss: 0.005439\t Accuracy:98.287%\n",
      "Epoch : 4 [27200/35700 (76%)]\tLoss: 0.090354\t Accuracy:98.289%\n",
      "Epoch : 4 [28800/35700 (81%)]\tLoss: 0.110525\t Accuracy:98.311%\n",
      "Epoch : 4 [30400/35700 (85%)]\tLoss: 0.041421\t Accuracy:98.318%\n",
      "Epoch : 4 [32000/35700 (90%)]\tLoss: 0.000071\t Accuracy:98.317%\n",
      "Epoch : 4 [33600/35700 (94%)]\tLoss: 0.161895\t Accuracy:98.332%\n",
      "Epoch : 4 [35200/35700 (99%)]\tLoss: 0.362992\t Accuracy:98.320%\n"
     ]
    }
   ],
   "source": [
    "fit(mlp, train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "cv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
