{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for MNIST Classification using PyTorch\n",
    "Generally follows this [tutorial](https://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/#comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3258, 0.7827, 0.9559],\n",
      "        [0.5913, 0.6803, 0.4966],\n",
      "        [0.0334, 0.4003, 0.3693],\n",
      "        [0.5823, 0.5097, 0.9807],\n",
      "        [0.0340, 0.7363, 0.0121]])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Read the training and test data into dataframes\n",
    "train_full = pd.read_csv('digit-recognizer/train.csv')\n",
    "test_full = pd.read_csv('digit-recognizer/test.csv')\n",
    "\n",
    "# Test Pytorch install\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Following Tutorial\n",
    "# * Variable class functionality deprecated in PyTorch, and all methods were attached to the base tensor class\n",
    "x = torch.ones(2, 2, requires_grad=True) * 2\n",
    "z = 2 * (x * x) + 5 * x\n",
    "z.backward(torch.ones(2,2))\n",
    "print(x.grad)\n",
    "\n",
    "# This isn't working correctly but proceeding anyway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Neural Net\n",
    "* Fully-connected means each neuron (i.e. element) in the input layer is connected to every element in the output layer via arbitrary weighting. So, if y is an element in the __output__ layer, $y = f(x_1, x_2, x_3, ..., x_n) = \\vec{W} \\bullet \\vec{X} $ where $x_i$ is the ith element in the __input__ layer, $\\vec{W}$ are the prescribed weights mapping input to output, and the input layer has __n__ elements\n",
    "\n",
    "* Note that this fully-connected layer is distinct from a convolution layer, where each element in the output layer is only dependent upon near neighbors in the input layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend the nn class\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 200) # fully-connected layer1, takes 28*28 pixel images and connects to 200 nodes\n",
    "        self.fc2 = nn.Linear(200, 200) # fully-connected layer2, takes 200 nodes and connects to 200 other nodes\n",
    "        self.fc3 = nn.Linear(200, 10)\n",
    "        \n",
    "    # This defines the forward propagation of the layers\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # ReLU is an activation function for the elements in the layer; Describes if they are activated based on provided values\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x) # log_softmax is another activation function\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't completely understand what's going on in the above cell, but just gonna keep following tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net) # Prints the net to verify the computational graph and the different layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ This is pretty neat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a stochastic gradient descent optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# create a loss function\n",
    "criterion = nn.NLLLoss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "cv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
